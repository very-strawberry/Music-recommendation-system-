# -*- coding: utf-8 -*-
"""MUSIC_REC_SYSTEM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MlrC7eLaN3tqh_nudmho9JCrrPdD-5WU
"""

# !pip install kaggle

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')


os.environ['KAGGLE_USERNAME'] = 'maiyaananya'
os.environ['KAGGLE_KEY'] = 'a7ff53fa6b8aef3261459d0f0cfb76be'

# Download the dataset from Kaggle

from kaggle.api.kaggle_api_extended import KaggleApi

api = KaggleApi()
api.authenticate()

# Download the specific dataset
dataset_name = "maharshipandya/-spotify-tracks-dataset"
api.dataset_download_files(dataset_name, path='.', unzip=True)

# Load the dataset
spotify_data = pd.read_csv('dataset.csv')

# Display basic information
print("Dataset shape:", spotify_data.shape)
print("\nFirst 5 rows:")
print(spotify_data.head())

# Clean the data
# Remove unnecessary columns
unnecessary_columns = ['track_id', 'album_id', 'album_name', 'track_genre', 'Unnamed: 0']
spotify_clean = spotify_data.drop(columns=unnecessary_columns, errors='ignore')

# Handle missing values and duplicates
spotify_clean = spotify_clean.dropna()
spotify_clean = spotify_clean.drop_duplicates(subset=['track_name', 'artists'])
spotify_clean = spotify_clean.reset_index(drop=True)

# Define reference columns and feature columns
reference_columns = ['track_name', 'artists']
feature_columns = ['duration_ms', 'explicit', 'danceability', 'energy',
                   'key', 'loudness', 'mode', 'speechiness', 'acousticness',
                   'instrumentalness', 'liveness', 'valence', 'tempo',
                   'time_signature', 'popularity']

print("\nNumber of features:", len(feature_columns))
print("Features:", feature_columns)

# Display the cleaned dataset with final columns
print("\nCleaned dataset shape:", spotify_clean.shape)
print("\nFinal columns:", spotify_clean.columns.tolist())

# Prepare feature matrix for SVD
feature_matrix = spotify_clean[feature_columns].copy()

# Scale the numerical features
numerical_features = ['duration_ms', 'danceability', 'energy', 'loudness',
                      'speechiness', 'acousticness', 'instrumentalness',
                      'liveness', 'valence', 'tempo', 'popularity']
scaler = StandardScaler()
feature_matrix[numerical_features] = scaler.fit_transform(feature_matrix[numerical_features])

# Split the data into training and testing sets
X_train, X_test = train_test_split(feature_matrix, test_size=0.2, random_state=42)

# First, apply SVD with all features (full rank)
n_components_full = len(feature_columns)
svd_full = TruncatedSVD(n_components=n_components_full, random_state=42)
X_train_svd_full = svd_full.fit_transform(X_train)
X_test_svd_full = svd_full.transform(X_test)

# Display eigenvalues and explained variance for all features
print("\n==== FULL SVD ANALYSIS ====")
print(f"Total number of components: {n_components_full}")

print("\nEigenvalues (singular values) for all features:")
print(svd_full.singular_values_)

# Calculate cumulative variance
cumulative_variance = np.cumsum(svd_full.explained_variance_ratio_)

# Create a DataFrame to display component information
components_df_full = pd.DataFrame({
    'Component': [f'Component {i+1}' for i in range(n_components_full)],
    'Eigenvalue': svd_full.singular_values_,
    'Explained Variance (%)': svd_full.explained_variance_ratio_ * 100,
    'Cumulative Variance (%)': cumulative_variance * 100
})
print("\nComponent analysis:")
print(components_df_full)

# Create a DataFrame of the components/eigenvectors
components_df = pd.DataFrame(svd_full.components_,
                           columns=feature_columns,
                           index=[f'Component {i+1}' for i in range(n_components_full)])
print("\nEigenvectors (components) for all features:")
print(components_df)

# ===== NEW CODE: Display all three matrices of SVD for full 15 features =====
print("\n==== FULL SVD MATRICES (15 FEATURES) ====")

# V^T matrix (Right singular vectors) - Already computed as svd_full.components_
print("\nV^T Matrix (Right Singular Vectors) - Shape:", svd_full.components_.shape)
print(pd.DataFrame(svd_full.components_,
                   index=[f'Component {i+1}' for i in range(n_components_full)],
                   columns=feature_columns))

# Sigma matrix (Singular values) - Already computed as svd_full.singular_values_
print("\nSigma Matrix (Singular Values) - Shape:", svd_full.singular_values_.shape)
sigma_full = np.zeros((n_components_full, n_components_full))
np.fill_diagonal(sigma_full, svd_full.singular_values_)
print(pd.DataFrame(sigma_full,
                   index=[f'Component {i+1}' for i in range(n_components_full)],
                   columns=[f'Component {i+1}' for i in range(n_components_full)]))

# U matrix (Left singular vectors) - Need to compute: X = U * Sigma * V^T
# SVD in sklearn doesn't store U, so we compute it: U = X * V * Sigma^(-1)
X_scaled = X_train.values
V = svd_full.components_.T
S_inv = np.zeros((n_components_full, n_components_full))
np.fill_diagonal(S_inv, 1/svd_full.singular_values_)
U_full = X_scaled @ V @ S_inv
print("\nU Matrix (Left Singular Vectors) - Shape:", U_full.shape)
print(pd.DataFrame(U_full[:10], columns=[f'Component {i+1}' for i in range(n_components_full)]))
print("(Showing first 10 rows only)")

# ============================================================================

# Function to interpret components based on their feature weights
def interpret_component(component_vector, feature_names, n_top_features=5):
    """Interpret what a component represents based on its top feature weights."""
    # Get the absolute values of the component vector
    abs_component = np.abs(component_vector)

    # Get indices of the top features
    top_indices = np.argsort(abs_component)[::-1][:n_top_features]

    # Get the signs of the top features
    top_signs = np.sign(component_vector[top_indices])

    # Get the feature names and their weights
    top_features = [(feature_names[i], component_vector[i]) for i in top_indices]

    interpretation = ""
    positive_features = []
    negative_features = []

    for feature, weight in top_features:
        if weight > 0:
            positive_features.append(f"{feature} ({weight:.3f})")
        else:
            negative_features.append(f"{feature} ({weight:.3f})")

    if positive_features:
        interpretation += "Positively correlated with: " + ", ".join(positive_features)

    if negative_features:
        if interpretation:
            interpretation += "\nNegatively correlated with: "
        else:
            interpretation += "Negatively correlated with: "
        interpretation += ", ".join(negative_features)

    # Add thematic interpretation
    if 'danceability' in [f[0] for f in top_features] and 'energy' in [f[0] for f in top_features]:
        interpretation += "\nPossible theme: Dance Energy"
    elif 'acousticness' in [f[0] for f in top_features] and 'instrumentalness' in [f[0] for f in top_features]:
        interpretation += "\nPossible theme: Acoustic/Instrumental Quality"
    elif 'valence' in [f[0] for f in top_features]:
        interpretation += "\nPossible theme: Musical Mood/Emotion"
    elif 'popularity' in [f[0] for f in top_features]:
        interpretation += "\nPossible theme: Commercial Appeal"
    elif 'tempo' in [f[0] for f in top_features]:
        interpretation += "\nPossible theme: Rhythm Characteristics"
    elif 'speechiness' in [f[0] for f in top_features]:
        interpretation += "\nPossible theme: Vocal Content"

    return interpretation

# Interpret each component
print("\n==== COMPONENT INTERPRETATIONS ====")
for i in range(n_components_full):
    print(f"\nComponent {i+1} (Explains {svd_full.explained_variance_ratio_[i]*100:.2f}% of variance):")
    interpretation = interpret_component(svd_full.components_[i], feature_columns)
    print(interpretation)

# Visualize the component interpretations for the top components
plt.figure(figsize=(15, 10))
for i in range(min(5, n_components_full)):  # Plot first 5 components
    component = svd_full.components_[i]
    plt.subplot(min(5, n_components_full), 1, i+1)

    # Sort features by absolute weight for better visualization
    sorted_idx = np.argsort(np.abs(component))
    plt.barh(np.array(feature_columns)[sorted_idx], component[sorted_idx])
    plt.title(f'Component {i+1} (Variance: {svd_full.explained_variance_ratio_[i]*100:.2f}%)')
    plt.tight_layout()
plt.savefig('component_interpretations.png')
plt.close()

# Determine the optimal number of components based on explained variance (90%)
optimal_components = np.where(cumulative_variance >= 0.90)[0][0] + 1
print(f"\nOptimal number of components (explaining 90% variance): {optimal_components}")

# Apply SVD with reduced dimensions
svd_reduced = TruncatedSVD(n_components=optimal_components, random_state=42)
X_train_svd = svd_reduced.fit_transform(X_train)
X_test_svd = svd_reduced.transform(X_test)

# Display information for reduced dimensions
print("\n==== REDUCED SVD ANALYSIS ====")
print(f"Reduced number of components: {optimal_components}")

print("\nEigenvalues (singular values) for reduced dimensions:")
print(svd_reduced.singular_values_)

print("\nCumulative explained variance (reduced):")
print(np.cumsum(svd_reduced.explained_variance_ratio_))

# Display eigenvectors (components) for reduced dimensions
components_df_reduced = pd.DataFrame(svd_reduced.components_,
                                   columns=feature_columns,
                                   index=[f'Component {i+1}' for i in range(optimal_components)])
print("\nEigenvectors (components) for reduced dimensions:")
print(components_df_reduced)

# ===== NEW CODE: Display all three matrices of SVD for reduced features =====
print("\n==== REDUCED SVD MATRICES ====")

# V^T matrix (Right singular vectors) - Already computed as svd_reduced.components_
print("\nV^T Matrix (Right Singular Vectors) - Shape:", svd_reduced.components_.shape)
print(pd.DataFrame(svd_reduced.components_,
                   index=[f'Component {i+1}' for i in range(optimal_components)],
                   columns=feature_columns))

# Sigma matrix (Singular values) - Already computed as svd_reduced.singular_values_
print("\nSigma Matrix (Singular Values) - Shape:", svd_reduced.singular_values_.shape)
sigma_reduced = np.zeros((optimal_components, optimal_components))
np.fill_diagonal(sigma_reduced, svd_reduced.singular_values_)
print(pd.DataFrame(sigma_reduced,
                   index=[f'Component {i+1}' for i in range(optimal_components)],
                   columns=[f'Component {i+1}' for i in range(optimal_components)]))

# U matrix (Left singular vectors) - Need to compute
X_scaled = X_train.values
V_reduced = svd_reduced.components_.T
S_inv_reduced = np.zeros((optimal_components, optimal_components))
np.fill_diagonal(S_inv_reduced, 1/svd_reduced.singular_values_)
U_reduced = X_scaled @ V_reduced @ S_inv_reduced
print("\nU Matrix (Left Singular Vectors) - Shape:", U_reduced.shape)
print(pd.DataFrame(U_reduced[:10], columns=[f'Component {i+1}' for i in range(optimal_components)]))
print("(Showing first 10 rows only)")

# ============================================================================

# Interpret the reduced components
print("\n==== REDUCED COMPONENT INTERPRETATIONS ====")
for i in range(optimal_components):
    print(f"\nComponent {i+1} (Explains {svd_reduced.explained_variance_ratio_[i]*100:.2f}% of variance):")
    interpretation = interpret_component(svd_reduced.components_[i], feature_columns)
    print(interpretation)

# Create a heatmap of component loadings for better visualization
plt.figure(figsize=(12, 8))
sns.heatmap(components_df_reduced, cmap='coolwarm', center=0, annot=True, fmt='.2f', linewidths=.5)
plt.title('Feature Weights in Each SVD Component')
plt.savefig('component_heatmap.png')
plt.close()

# Transform the entire feature matrix using the trained SVD model
full_svd = svd_reduced.transform(feature_matrix)

# ---- Recommendation Functions ----

def recommend_by_title(title, n_recommendations=10):
    # Check if the title exists in the dataset
    title_matches = spotify_clean[spotify_clean['track_name'].str.lower() == title.lower()]

    if len(title_matches) == 0:
        # Try to find a partial match
        title_contains = spotify_clean[spotify_clean['track_name'].str.lower().str.contains(title.lower())]
        if len(title_contains) == 0:
            return f"No tracks found matching '{title}'. Please try another title."

        # Take the first match
        idx = title_contains.index[0]
        matched_title = title_contains.iloc[0]['track_name']
        print(f"Exact match not found. Using closest match: '{matched_title}'")
    else:
        idx = title_matches.index[0]
        matched_title = title_matches.iloc[0]['track_name']

    # Get the SVD representation of the track
    track_svd = full_svd[idx].reshape(1, -1)

    # Calculate similarity with all tracks
    sim_scores = cosine_similarity(track_svd, full_svd)
    sim_scores = sim_scores.flatten()

    # Get top similar tracks excluding the input track
    sim_scores_indices = np.argsort(sim_scores)[::-1]
    sim_scores_indices = sim_scores_indices[sim_scores_indices != idx][:n_recommendations]

    # Return recommended tracks
    recommended_tracks = spotify_clean.iloc[sim_scores_indices][['track_name', 'artists', 'popularity']]
    recommended_tracks['similarity_score'] = sim_scores[sim_scores_indices]
    return recommended_tracks

def recommend_by_artist(artist, n_recommendations=10):
    # Check if the artist exists in the dataset
    artist_matches = spotify_clean[spotify_clean['artists'].str.lower() == artist.lower()]

    if len(artist_matches) == 0:
        # Try to find a partial match
        artist_contains = spotify_clean[spotify_clean['artists'].str.lower().str.contains(artist.lower())]
        if len(artist_contains) == 0:
            return f"No artists found matching '{artist}'. Please try another artist."

        # Take the first match
        matched_artist = artist_contains.iloc[0]['artists']
        print(f"Exact match not found. Using closest match: '{matched_artist}'")
        artist_matches = artist_contains

    # Get all tracks by this artist
    artist_indices_list = artist_matches.index.tolist()

    if not artist_indices_list:
        return f"No tracks found for artist '{artist}'. Please try another artist."

    # Average the SVD vectors for all tracks by this artist
    artist_svd = np.mean(full_svd[artist_indices_list], axis=0).reshape(1, -1)

    # Calculate similarity with all tracks
    sim_scores = cosine_similarity(artist_svd, full_svd)
    sim_scores = sim_scores.flatten()

    # Remove the artist's own tracks
    mask = np.ones(len(sim_scores), dtype=bool)
    mask[artist_indices_list] = False
    filtered_indices = np.array(range(len(sim_scores)))[mask]
    filtered_scores = sim_scores[mask]

    # Sort and get top recommendations
    sim_scores_indices = filtered_scores.argsort()[::-1][:n_recommendations]
    rec_indices = filtered_indices[sim_scores_indices]

    # Return recommended tracks
    recommended_tracks = spotify_clean.iloc[rec_indices][['track_name', 'artists', 'popularity']]
    recommended_tracks['similarity_score'] = sim_scores[rec_indices]
    return recommended_tracks

def recommend_by_mood(mood, n_recommendations=10):
    # Define mood profiles based on audio features
    mood_profiles = {
        'happy': {'valence': 0.8, 'energy': 0.7, 'danceability': 0.7, 'tempo': 0.6},
        'sad': {'valence': 0.2, 'energy': 0.3, 'acousticness': 0.7, 'tempo': -0.3},
        'energetic': {'energy': 0.9, 'tempo': 0.8, 'loudness': 0.7, 'danceability': 0.7},
        'relaxed': {'acousticness': 0.8, 'energy': 0.3, 'instrumentalness': 0.6, 'tempo': -0.5},
        'chill': {'acousticness': 0.6, 'energy': 0.4, 'valence': 0.6, 'tempo': -0.2}
    }

    if mood.lower() not in mood_profiles:
        return f"Mood '{mood}' not recognized. Available moods: {list(mood_profiles.keys())}"

    # Create a feature vector for the selected mood
    mood_vector = np.zeros(len(feature_columns))
    for feature, value in mood_profiles[mood.lower()].items():
        if feature in feature_columns:
            feature_index = feature_columns.index(feature)
            mood_vector[feature_index] = value

    # Transform the mood vector into the SVD space
    mood_svd = svd_reduced.transform(mood_vector.reshape(1, -1))

    # Calculate similarity with all tracks
    sim_scores = cosine_similarity(mood_svd, full_svd)
    sim_scores = sim_scores.flatten()
    sim_scores_indices = sim_scores.argsort()[::-1][:n_recommendations]

    # Return recommended tracks
    recommended_tracks = spotify_clean.iloc[sim_scores_indices][['track_name', 'artists', 'popularity']]
    recommended_tracks['similarity_score'] = sim_scores[sim_scores_indices]
    return recommended_tracks

# Evaluate the model
print("\n==== MODEL VALIDATION ====")
print(f"Number of components used: {optimal_components}")
print(f"Total variance explained: {np.sum(svd_reduced.explained_variance_ratio_):.4f}")

# Reconstruction error
X_train_reconstructed = svd_reduced.inverse_transform(X_train_svd)
reconstruction_error = mean_squared_error(X_train, X_train_reconstructed)
print(f"Training reconstruction error (MSE): {reconstruction_error:.4f}")

X_test_reconstructed = svd_reduced.inverse_transform(X_test_svd)
test_reconstruction_error = mean_squared_error(X_test, X_test_reconstructed)
print(f"Test reconstruction error (MSE): {test_reconstruction_error:.4f}")

# Feature importance analysis
print("\n==== FEATURE IMPORTANCE ANALYSIS ====")
feature_importance = np.sum(np.abs(svd_reduced.components_), axis=0)
feature_importance = feature_importance / np.sum(feature_importance)  # Normalize
feature_importance_df = pd.DataFrame({
    'Feature': feature_columns,
    'Importance': feature_importance
})
feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)
print(feature_importance_df)

# Create a visualization of feature importance
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Overall Feature Importance in SVD Model')
plt.tight_layout()
plt.savefig('feature_importance.png')
plt.close()

# Example recommendations
print("\n==== EXAMPLE RECOMMENDATIONS ====")
print("\nExample recommendations by track title:")
sample_track = spotify_clean['track_name'].iloc[20]
print(f"Sample track: {sample_track}")
print(recommend_by_title(sample_track, 5))

print("\nExample recommendations by artist:")
sample_artist = spotify_clean['artists'].iloc[20]
print(f"Sample artist: {sample_artist}")
print(recommend_by_artist(sample_artist, 5))

print("\nExample recommendations by mood:")
print("Recommendations for 'happy' mood:")
print(recommend_by_mood('happy', 5))

# Install necessary packages if not already installed
# !pip install kaggle

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')


os.environ['KAGGLE_USERNAME'] = 'maiyaananya'
os.environ['KAGGLE_KEY'] = 'a7ff53fa6b8aef3261459d0f0cfb76be'

# Download the dataset from Kaggle

from kaggle.api.kaggle_api_extended import KaggleApi

api = KaggleApi()
api.authenticate()

# Download the specific dataset
dataset_name = "maharshipandya/-spotify-tracks-dataset"
api.dataset_download_files(dataset_name, path='.', unzip=True)

# Load the dataset
spotify_data = pd.read_csv('dataset.csv')

# Display basic information
print("Dataset shape:", spotify_data.shape)
print("\nFirst 5 rows:")
print(spotify_data.head())

# Clean the data
# Remove unnecessary columns
unnecessary_columns = ['track_id', 'album_id', 'album_name', 'track_genre', 'Unnamed: 0']
spotify_clean = spotify_data.drop(columns=unnecessary_columns, errors='ignore')

# Handle missing values and duplicates
spotify_clean = spotify_clean.dropna()
spotify_clean = spotify_clean.drop_duplicates(subset=['track_name', 'artists'])
spotify_clean = spotify_clean.reset_index(drop=True)

# Define reference columns and feature columns
reference_columns = ['track_name', 'artists']
feature_columns = ['duration_ms', 'explicit', 'danceability', 'energy',
                   'key', 'loudness', 'mode', 'speechiness', 'acousticness',
                   'instrumentalness', 'liveness', 'valence', 'tempo',
                   'time_signature', 'popularity']

print("\nNumber of features:", len(feature_columns))
print("Features:", feature_columns)

# Display the cleaned dataset with final columns
print("\nCleaned dataset shape:", spotify_clean.shape)
print("\nFinal columns:", spotify_clean.columns.tolist())

# Prepare feature matrix for SVD
feature_matrix = spotify_clean[feature_columns].copy()

# Scale the numerical features
numerical_features = ['duration_ms', 'danceability', 'energy', 'loudness',
                      'speechiness', 'acousticness', 'instrumentalness',
                      'liveness', 'valence', 'tempo', 'popularity']
scaler = StandardScaler()
feature_matrix[numerical_features] = scaler.fit_transform(feature_matrix[numerical_features])

# Split the data into training and testing sets
X_train, X_test = train_test_split(feature_matrix, test_size=0.2, random_state=42)

# First, apply SVD with all features (full rank)
n_components_full = len(feature_columns)
svd_full = TruncatedSVD(n_components=n_components_full, random_state=42)
X_train_svd_full = svd_full.fit_transform(X_train)
X_test_svd_full = svd_full.transform(X_test)

# Display eigenvalues and explained variance for all features
print("\n==== FULL SVD ANALYSIS ====")
print(f"Total number of components: {n_components_full}")

print("\nEigenvalues (singular values) for all features:")
print(svd_full.singular_values_)

# Calculate cumulative variance
cumulative_variance = np.cumsum(svd_full.explained_variance_ratio_)

# Create a DataFrame to display component information
components_df_full = pd.DataFrame({
    'Component': [f'Component {i+1}' for i in range(n_components_full)],
    'Eigenvalue': svd_full.singular_values_,
    'Explained Variance (%)': svd_full.explained_variance_ratio_ * 100,
    'Cumulative Variance (%)': cumulative_variance * 100
})
print("\nComponent analysis:")
print(components_df_full)

# Create a DataFrame of the components/eigenvectors
components_df = pd.DataFrame(svd_full.components_,
                           columns=feature_columns,
                           index=[f'Component {i+1}' for i in range(n_components_full)])
print("\nEigenvectors (components) for all features:")
print(components_df)

# ===== NEW CODE: Display all three matrices of SVD for full 15 features =====
print("\n==== FULL SVD MATRICES (15 FEATURES) ====")

# V^T matrix (Right singular vectors) - Already computed as svd_full.components_
print("\nV^T Matrix (Right Singular Vectors) - Shape:", svd_full.components_.shape)
print(pd.DataFrame(svd_full.components_,
                   index=[f'Component {i+1}' for i in range(n_components_full)],
                   columns=feature_columns))

# Sigma matrix (Singular values) - Already computed as svd_full.singular_values_
print("\nSigma Matrix (Singular Values) - Shape:", svd_full.singular_values_.shape)
sigma_full = np.zeros((n_components_full, n_components_full))
np.fill_diagonal(sigma_full, svd_full.singular_values_)
print(pd.DataFrame(sigma_full,
                   index=[f'Component {i+1}' for i in range(n_components_full)],
                   columns=[f'Component {i+1}' for i in range(n_components_full)]))

# U matrix (Left singular vectors) - Need to compute: X = U * Sigma * V^T
# SVD in sklearn doesn't store U, so we compute it: U = X * V * Sigma^(-1)
X_scaled = X_train.values
V = svd_full.components_.T
S_inv = np.zeros((n_components_full, n_components_full))
np.fill_diagonal(S_inv, 1/svd_full.singular_values_)
U_full = X_scaled @ V @ S_inv
print("\nU Matrix (Left Singular Vectors) - Shape:", U_full.shape)
print(pd.DataFrame(U_full[:10], columns=[f'Component {i+1}' for i in range(n_components_full)]))
print("(Showing first 10 rows only)")

# ============================================================================

# Function to interpret components based on their feature weights
def interpret_component(component_vector, feature_names, n_top_features=5):
    """Interpret what a component represents based on its top feature weights."""
    # Get the absolute values of the component vector
    abs_component = np.abs(component_vector)

    # Get indices of the top features
    top_indices = np.argsort(abs_component)[::-1][:n_top_features]

    # Get the signs of the top features
    top_signs = np.sign(component_vector[top_indices])

    # Get the feature names and their weights
    top_features = [(feature_names[i], component_vector[i]) for i in top_indices]

    interpretation = ""
    positive_features = []
    negative_features = []

    for feature, weight in top_features:
        if weight > 0:
            positive_features.append(f"{feature} ({weight:.3f})")
        else:
            negative_features.append(f"{feature} ({weight:.3f})")

    if positive_features:
        interpretation += "Positively correlated with: " + ", ".join(positive_features)

    if negative_features:
        if interpretation:
            interpretation += "\nNegatively correlated with: "
        else:
            interpretation += "Negatively correlated with: "
        interpretation += ", ".join(negative_features)

    # Add thematic interpretation
    if 'danceability' in [f[0] for f in top_features] and 'energy' in [f[0] for f in top_features]:
        interpretation += "\nPossible theme: Dance Energy"
    elif 'acousticness' in [f[0] for f in top_features] and 'instrumentalness' in [f[0] for f in top_features]:
        interpretation += "\nPossible theme: Acoustic/Instrumental Quality"
    elif 'valence' in [f[0] for f in top_features]:
        interpretation += "\nPossible theme: Musical Mood/Emotion"
    elif 'popularity' in [f[0] for f in top_features]:
        interpretation += "\nPossible theme: Commercial Appeal"
    elif 'tempo' in [f[0] for f in top_features]:
        interpretation += "\nPossible theme: Rhythm Characteristics"
    elif 'speechiness' in [f[0] for f in top_features]:
        interpretation += "\nPossible theme: Vocal Content"

    return interpretation

# Interpret each component
print("\n==== COMPONENT INTERPRETATIONS ====")
for i in range(n_components_full):
    print(f"\nComponent {i+1} (Explains {svd_full.explained_variance_ratio_[i]*100:.2f}% of variance):")
    interpretation = interpret_component(svd_full.components_[i], feature_columns)
    print(interpretation)

# Visualize the component interpretations for the top components
plt.figure(figsize=(15, 10))
for i in range(min(5, n_components_full)):  # Plot first 5 components
    component = svd_full.components_[i]
    plt.subplot(min(5, n_components_full), 1, i+1)

    # Sort features by absolute weight for better visualization
    sorted_idx = np.argsort(np.abs(component))
    plt.barh(np.array(feature_columns)[sorted_idx], component[sorted_idx])
    plt.title(f'Component {i+1} (Variance: {svd_full.explained_variance_ratio_[i]*100:.2f}%)')
    plt.tight_layout()
plt.savefig('component_interpretations.png')
plt.close()

# Determine the optimal number of components based on explained variance (90%)
optimal_components = np.where(cumulative_variance >= 0.90)[0][0] + 1
print(f"\nOptimal number of components (explaining 90% variance): {optimal_components}")

# Apply SVD with reduced dimensions
svd_reduced = TruncatedSVD(n_components=optimal_components, random_state=42)
X_train_svd = svd_reduced.fit_transform(X_train)
X_test_svd = svd_reduced.transform(X_test)

# Display information for reduced dimensions
print("\n==== REDUCED SVD ANALYSIS ====")
print(f"Reduced number of components: {optimal_components}")

print("\nEigenvalues (singular values) for reduced dimensions:")
print(svd_reduced.singular_values_)

print("\nCumulative explained variance (reduced):")
print(np.cumsum(svd_reduced.explained_variance_ratio_))

# Display eigenvectors (components) for reduced dimensions
components_df_reduced = pd.DataFrame(svd_reduced.components_,
                                   columns=feature_columns,
                                   index=[f'Component {i+1}' for i in range(optimal_components)])
print("\nEigenvectors (components) for reduced dimensions:")
print(components_df_reduced)

# ===== NEW CODE: Display all three matrices of SVD for reduced features =====
print("\n==== REDUCED SVD MATRICES ====")

# V^T matrix (Right singular vectors) - Already computed as svd_reduced.components_
print("\nV^T Matrix (Right Singular Vectors) - Shape:", svd_reduced.components_.shape)
print(pd.DataFrame(svd_reduced.components_,
                   index=[f'Component {i+1}' for i in range(optimal_components)],
                   columns=feature_columns))

# Sigma matrix (Singular values) - Already computed as svd_reduced.singular_values_
print("\nSigma Matrix (Singular Values) - Shape:", svd_reduced.singular_values_.shape)
sigma_reduced = np.zeros((optimal_components, optimal_components))
np.fill_diagonal(sigma_reduced, svd_reduced.singular_values_)
print(pd.DataFrame(sigma_reduced,
                   index=[f'Component {i+1}' for i in range(optimal_components)],
                   columns=[f'Component {i+1}' for i in range(optimal_components)]))

# U matrix (Left singular vectors) - Need to compute
X_scaled = X_train.values
V_reduced = svd_reduced.components_.T
S_inv_reduced = np.zeros((optimal_components, optimal_components))
np.fill_diagonal(S_inv_reduced, 1/svd_reduced.singular_values_)
U_reduced = X_scaled @ V_reduced @ S_inv_reduced
print("\nU Matrix (Left Singular Vectors) - Shape:", U_reduced.shape)
print(pd.DataFrame(U_reduced[:10], columns=[f'Component {i+1}' for i in range(optimal_components)]))
print("(Showing first 10 rows only)")

# ============================================================================

# Interpret the reduced components
print("\n==== REDUCED COMPONENT INTERPRETATIONS ====")
for i in range(optimal_components):
    print(f"\nComponent {i+1} (Explains {svd_reduced.explained_variance_ratio_[i]*100:.2f}% of variance):")
    interpretation = interpret_component(svd_reduced.components_[i], feature_columns)
    print(interpretation)

# Create a heatmap of component loadings for better visualization
plt.figure(figsize=(12, 8))
sns.heatmap(components_df_reduced, cmap='coolwarm', center=0, annot=True, fmt='.2f', linewidths=.5)
plt.title('Feature Weights in Each SVD Component')
plt.savefig('component_heatmap.png')
plt.close()

# Transform the entire feature matrix using the trained SVD model
full_svd = svd_reduced.transform(feature_matrix)

# ---- Recommendation Functions ----

def recommend_by_title(title, n_recommendations=10):
    # Check if the title exists in the dataset
    title_matches = spotify_clean[spotify_clean['track_name'].str.lower() == title.lower()]

    if len(title_matches) == 0:
        # Try to find a partial match
        title_contains = spotify_clean[spotify_clean['track_name'].str.lower().str.contains(title.lower())]
        if len(title_contains) == 0:
            return f"No tracks found matching '{title}'. Please try another title."

        # Take the first match
        idx = title_contains.index[0]
        matched_title = title_contains.iloc[0]['track_name']
        print(f"Exact match not found. Using closest match: '{matched_title}'")
    else:
        idx = title_matches.index[0]
        matched_title = title_matches.iloc[0]['track_name']

    # Get the SVD representation of the track
    track_svd = full_svd[idx].reshape(1, -1)

    # Calculate similarity with all tracks
    sim_scores = cosine_similarity(track_svd, full_svd)
    sim_scores = sim_scores.flatten()

    # Get top similar tracks excluding the input track
    sim_scores_indices = np.argsort(sim_scores)[::-1]
    sim_scores_indices = sim_scores_indices[sim_scores_indices != idx][:n_recommendations]

    # Return recommended tracks
    recommended_tracks = spotify_clean.iloc[sim_scores_indices][['track_name', 'artists', 'popularity']]
    recommended_tracks['similarity_score'] = sim_scores[sim_scores_indices]
    return recommended_tracks

def recommend_by_artist(artist, n_recommendations=10):
    # Check if the artist exists in the dataset
    artist_matches = spotify_clean[spotify_clean['artists'].str.lower() == artist.lower()]

    if len(artist_matches) == 0:
        # Try to find a partial match
        artist_contains = spotify_clean[spotify_clean['artists'].str.lower().str.contains(artist.lower())]
        if len(artist_contains) == 0:
            return f"No artists found matching '{artist}'. Please try another artist."

        # Take the first match
        matched_artist = artist_contains.iloc[0]['artists']
        print(f"Exact match not found. Using closest match: '{matched_artist}'")
        artist_matches = artist_contains

    # Get all tracks by this artist
    artist_indices_list = artist_matches.index.tolist()

    if not artist_indices_list:
        return f"No tracks found for artist '{artist}'. Please try another artist."

    # Average the SVD vectors for all tracks by this artist
    artist_svd = np.mean(full_svd[artist_indices_list], axis=0).reshape(1, -1)

    # Calculate similarity with all tracks
    sim_scores = cosine_similarity(artist_svd, full_svd)
    sim_scores = sim_scores.flatten()

    # Remove the artist's own tracks
    mask = np.ones(len(sim_scores), dtype=bool)
    mask[artist_indices_list] = False
    filtered_indices = np.array(range(len(sim_scores)))[mask]
    filtered_scores = sim_scores[mask]

    # Sort and get top recommendations
    sim_scores_indices = filtered_scores.argsort()[::-1][:n_recommendations]
    rec_indices = filtered_indices[sim_scores_indices]

    # Return recommended tracks
    recommended_tracks = spotify_clean.iloc[rec_indices][['track_name', 'artists', 'popularity']]
    recommended_tracks['similarity_score'] = sim_scores[rec_indices]
    return recommended_tracks

def recommend_by_mood(mood, n_recommendations=10):
    # Define mood profiles based on audio features
    mood_profiles = {
        'happy': {'valence': 0.8, 'energy': 0.7, 'danceability': 0.7, 'tempo': 0.6},
        'sad': {'valence': 0.2, 'energy': 0.3, 'acousticness': 0.7, 'tempo': -0.3},
        'energetic': {'energy': 0.9, 'tempo': 0.8, 'loudness': 0.7, 'danceability': 0.7},
        'relaxed': {'acousticness': 0.8, 'energy': 0.3, 'instrumentalness': 0.6, 'tempo': -0.5},
        'chill': {'acousticness': 0.6, 'energy': 0.4, 'valence': 0.6, 'tempo': -0.2}
    }

    if mood.lower() not in mood_profiles:
        return f"Mood '{mood}' not recognized. Available moods: {list(mood_profiles.keys())}"

    # Create a feature vector for the selected mood
    mood_vector = np.zeros(len(feature_columns))
    for feature, value in mood_profiles[mood.lower()].items():
        if feature in feature_columns:
            feature_index = feature_columns.index(feature)
            mood_vector[feature_index] = value

    # Transform the mood vector into the SVD space
    mood_svd = svd_reduced.transform(mood_vector.reshape(1, -1))

    # Calculate similarity with all tracks
    sim_scores = cosine_similarity(mood_svd, full_svd)
    sim_scores = sim_scores.flatten()
    sim_scores_indices = sim_scores.argsort()[::-1][:n_recommendations]

    # Return recommended tracks
    recommended_tracks = spotify_clean.iloc[sim_scores_indices][['track_name', 'artists', 'popularity']]
    recommended_tracks['similarity_score'] = sim_scores[sim_scores_indices]
    return recommended_tracks

# Evaluate the model
print("\n==== MODEL VALIDATION ====")
print(f"Number of components used: {optimal_components}")
print(f"Total variance explained: {np.sum(svd_reduced.explained_variance_ratio_):.4f}")

# Reconstruction error
X_train_reconstructed = svd_reduced.inverse_transform(X_train_svd)
reconstruction_error = mean_squared_error(X_train, X_train_reconstructed)
print(f"Training reconstruction error (MSE): {reconstruction_error:.4f}")

X_test_reconstructed = svd_reduced.inverse_transform(X_test_svd)
test_reconstruction_error = mean_squared_error(X_test, X_test_reconstructed)
print(f"Test reconstruction error (MSE): {test_reconstruction_error:.4f}")

# Feature importance analysis
print("\n==== FEATURE IMPORTANCE ANALYSIS ====")
feature_importance = np.sum(np.abs(svd_reduced.components_), axis=0)
feature_importance = feature_importance / np.sum(feature_importance)  # Normalize
feature_importance_df = pd.DataFrame({
    'Feature': feature_columns,
    'Importance': feature_importance
})
feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)
print(feature_importance_df)

# Create a visualization of feature importance
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Overall Feature Importance in SVD Model')
plt.tight_layout()
plt.savefig('feature_importance.png')
plt.close()

# ===== NEW CODE: ANALYZE EFFECT OF DIFFERENT DIMENSIONALITIES =====
print("\n==== ANALYZING EFFECT OF DIFFERENT DIMENSIONALITIES ON MODEL VALIDATION ====")

# Define the dimensionalities we want to analyze
dimensionalities = [1, 2, 3, 5, 7, 10, optimal_components, n_components_full]
dimensionalities = sorted(list(set(dimensionalities)))  # Remove duplicates and sort

# Arrays to store results
train_errors = []
test_errors = []
explained_variances = []

# Sample track for recommendation comparison
sample_track_idx = 20
sample_track = spotify_clean['track_name'].iloc[sample_track_idx]

# Create a list to store recommendations for each dimensionality
recommendations_by_dim = []

# Function to transform data and compute recommendations with specific dimensionality
def get_recommendations_with_dim(n_dim, title_idx, n_rec=5):
    # Create SVD model with specific dimensionality
    svd_model = TruncatedSVD(n_components=n_dim, random_state=42)
    train_svd = svd_model.fit_transform(X_train)

    # Transform full data
    full_data_svd = svd_model.transform(feature_matrix)

    # Get the SVD representation of the track
    track_svd = full_data_svd[title_idx].reshape(1, -1)

    # Calculate similarity
    sim_scores = cosine_similarity(track_svd, full_data_svd)
    sim_scores = sim_scores.flatten()

    # Get top similar tracks excluding the input track
    sim_indices = np.argsort(sim_scores)[::-1]
    sim_indices = sim_indices[sim_indices != title_idx][:n_rec]

    # Return recommended tracks
    rec_tracks = spotify_clean.iloc[sim_indices][['track_name', 'artists']]
    rec_tracks['similarity_score'] = sim_scores[sim_indices]
    return rec_tracks

# Analyze each dimensionality
for dim in dimensionalities:
    print(f"\n---- Using {dim} components ----")

    # Create and fit SVD model with specific dimensionality
    svd_model = TruncatedSVD(n_components=dim, random_state=42)
    X_train_svd_dim = svd_model.fit_transform(X_train)
    X_test_svd_dim = svd_model.transform(X_test)

    # Calculate explained variance
    explained_var = np.sum(svd_model.explained_variance_ratio_)
    explained_variances.append(explained_var)
    print(f"Total explained variance: {explained_var:.4f}")

    # Calculate reconstruction error
    X_train_reconstructed = svd_model.inverse_transform(X_train_svd_dim)
    train_mse = mean_squared_error(X_train, X_train_reconstructed)
    train_errors.append(train_mse)
    print(f"Training reconstruction error (MSE): {train_mse:.4f}")

    X_test_reconstructed = svd_model.inverse_transform(X_test_svd_dim)
    test_mse = mean_squared_error(X_test, X_test_reconstructed)
    test_errors.append(test_mse)
    print(f"Test reconstruction error (MSE): {test_mse:.4f}")

    # Get recommendations for sample track
    if dim in [1, 3, optimal_components, n_components_full]:  # Just show for select dimensions to save space
        print(f"\nRecommendations using {dim} components for track '{sample_track}':")
        rec = get_recommendations_with_dim(dim, sample_track_idx, 3)
        print(rec)
        recommendations_by_dim.append((dim, rec))

# Visualize the results
plt.figure(figsize=(12, 10))

# Plot 1: Reconstruction Error vs Dimensionality
plt.subplot(2, 1, 1)
plt.plot(dimensionalities, train_errors, 'b-o', label='Training Error')
plt.plot(dimensionalities, test_errors, 'r-o', label='Test Error')
plt.axvline(x=optimal_components, color='g', linestyle='--', label=f'Optimal Components ({optimal_components})')
plt.xlabel('Number of Components')
plt.ylabel('Reconstruction Error (MSE)')
plt.title('Reconstruction Error vs Dimensionality')
plt.legend()
plt.grid(True)

# Plot 2: Explained Variance vs Dimensionality
plt.subplot(2, 1, 2)
plt.plot(dimensionalities, explained_variances, 'g-o')
plt.axvline(x=optimal_components, color='r', linestyle='--', label=f'Optimal Components ({optimal_components})')
plt.xlabel('Number of Components')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance vs Dimensionality')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.savefig('dimensionality_analysis.png')
plt.close()

# Create visualization for individual component effects
plt.figure(figsize=(15, 12))
n_display = min(5, n_components_full)

# For each of the top components, show what happens when only that component is used
for i in range(n_display):
    # Create SVD model with all components
    svd_all = TruncatedSVD(n_components=n_components_full, random_state=42)
    X_train_svd_all = svd_all.fit_transform(X_train)

    # Create data with only one component
    X_train_one_comp = np.zeros_like(X_train_svd_all)
    X_train_one_comp[:, i] = X_train_svd_all[:, i]  # Keep only the i-th component

    # Reconstruct data
    X_train_reconstructed = svd_all.inverse_transform(X_train_one_comp)

    # Calculate reconstruction error
    single_comp_error = mean_squared_error(X_train, X_train_reconstructed)

    # Plot reconstruction comparison for a sample data point
    sample_idx = 0  # Use first sample in training set
    plt.subplot(n_display, 1, i+1)

    # Get original and reconstructed values for the sample
    original_values = X_train.iloc[sample_idx].values
    reconstructed_values = X_train_reconstructed[sample_idx]

    # Create x-axis labels
    x = np.arange(len(feature_columns))
    width = 0.35

    # Plot original vs reconstructed
    plt.bar(x - width/2, original_values, width, label='Original')
    plt.bar(x + width/2, reconstructed_values, width, label='Using only Component ' + str(i+1))

    plt.xlabel('Features')
    plt.ylabel('Scaled Value')
    plt.title(f'Component {i+1} Only (Explains {svd_all.explained_variance_ratio_[i]*100:.2f}% variance, Reconstruction Error: {single_comp_error:.4f})')
    plt.xticks(x, feature_columns, rotation=90)
    plt.legend(loc='upper right')
    plt.grid(True, axis='y')
    plt.tight_layout()

plt.savefig('individual_component_effects.png')
plt.close()

# Example recommendations
print("\n==== EXAMPLE RECOMMENDATIONS ====")
print("\nExample recommendations by track title:")
sample_track = spotify_clean['track_name'].iloc[20]
print(f"Sample track: {sample_track}")
print(recommend_by_title(sample_track, 5))

print("\nExample recommendations by artist:")
sample_artist = spotify_clean['artists'].iloc[20]
print(f"Sample artist: {sample_artist}")
print(recommend_by_artist(sample_artist, 5))

print("\nExample recommendations by mood:")
print("Recommendations for 'happy' mood:")
print(recommend_by_mood('happy', 5))

# Print summary of dimensionality analysis
print("\n==== DIMENSIONALITY ANALYSIS SUMMARY ====")
summary_df = pd.DataFrame({
    'Dimensionality': dimensionalities,
    'Explained Variance': explained_variances,
    'Training Error': train_errors,
    'Test Error': test_errors
})
print(summary_df)
print("\nObservations:")
print("1. Lower dimensionality (fewer components) results in higher reconstruction error")
print("2. As dimensionality increases, explained variance increases and reconstruction error decreases")
print("3. The optimal number of components balances complexity with performance")
print(f"4. Using all {n_components_full} components explains 100% of variance but may overfit")